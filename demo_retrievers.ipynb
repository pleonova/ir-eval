{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval: BM25 vs Jina Embeddings v4\n",
    "\n",
    "This notebook compares two retrieval approaches:\n",
    "- **BM25**: Traditional lexical (keyword-based) retrieval\n",
    "- **Jina Embeddings v4**: Modern semantic (meaning-based) retrieval\n",
    "\n",
    "We'll explore their strengths and weaknesses with carefully designed examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%%capture\n",
    "%pip install pandas==2.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulaleonova/repos/ir-research/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/paulaleonova/repos/ir-research/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from retrievers.embeddings import JinaEmbedder, DummyEmbedder, EmbeddingRetriever\n",
    "from retrievers.bm25 import BM25\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Corpus\n",
    "\n",
    "We've designed this corpus to highlight different retrieval scenarios:\n",
    "1. **Exact keyword matches** - BM25 should excel\n",
    "2. **Semantic/paraphrased queries** - Embeddings should excel\n",
    "3. **Technical terms** - BM25's strength with rare terms\n",
    "4. **Conceptual understanding** - Embeddings' strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    {\n",
    "        \"doc_id\": \"climate1\",\n",
    "        \"text\": \"Climate change is causing rising sea levels and extreme weather events, \"\n",
    "               \"threatening coastal cities worldwide. Scientists warn of increasing hurricanes and floods.\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"climate2\", \n",
    "        \"text\": \"Renewable energy sources like solar and wind power are essential for \"\n",
    "               \"reducing greenhouse gas emissions and combating global warming.\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"ml1\",\n",
    "        \"text\": \"Machine learning algorithms can identify patterns in large datasets and \"\n",
    "               \"make predictions based on training data using neural networks.\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"ml2\",\n",
    "        \"text\": \"Deep neural networks use multiple layers to learn hierarchical \"\n",
    "               \"representations of data for complex tasks like image recognition.\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"ml3\",\n",
    "        \"text\": \"Artificial intelligence systems can now understand natural language, \"\n",
    "               \"recognize objects in images, and even generate creative content.\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"space1\",\n",
    "        \"text\": \"The James Webb Space Telescope is revealing unprecedented details about \"\n",
    "               \"distant galaxies and the early universe using infrared technology.\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"quantum1\",\n",
    "        \"text\": \"Quantum computing leverages quantum entanglement and superposition to solve \"\n",
    "               \"complex computational problems exponentially faster than classical computers.\"\n",
    "    },\n",
    "    {\n",
    "        \"doc_id\": \"bio1\",\n",
    "        \"text\": \"CRISPR gene editing technology enables precise modifications to DNA sequences, \"\n",
    "               \"opening new possibilities for treating genetic diseases and improving crops.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Display corpus\n",
    "df_corpus = pd.DataFrame(corpus)\n",
    "print(f\"üìö Corpus: {len(corpus)} documents\\n\")\n",
    "df_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Queries\n",
    "\n",
    "Each query is designed to test specific retrieval characteristics:\n",
    "\n",
    "### BM25 Strengths:\n",
    "- **Exact keywords** (queries 1, 4, 5)\n",
    "- **Rare technical terms**\n",
    "\n",
    "### Embedding Strengths:\n",
    "- **Semantic similarity** (queries 2, 7)\n",
    "- **Paraphrasing** (query 6)\n",
    "- **Conceptual understanding** (query 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    {\n",
    "        \"id\": \"q1\",\n",
    "        \"query\": \"quantum entanglement superposition\",\n",
    "        \"expected\": \"quantum1\",\n",
    "        \"type\": \"Exact keywords (BM25 strength)\",\n",
    "        \"description\": \"Contains rare, technical terms that appear in only one document\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q2\",\n",
    "        \"query\": \"global warming and rising temperatures\",\n",
    "        \"expected\": \"climate2\",\n",
    "        \"type\": \"Semantic similarity (Embedding strength)\",\n",
    "        \"description\": \"'global warming' is semantically similar to 'greenhouse gas emissions'\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q3\",\n",
    "        \"query\": \"How do computers learn from data?\",\n",
    "        \"expected\": \"ml1\",\n",
    "        \"type\": \"Conceptual question (Embedding strength)\",\n",
    "        \"description\": \"Question form, no exact keywords but conceptually about machine learning\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q4\",\n",
    "        \"query\": \"CRISPR DNA editing\",\n",
    "        \"expected\": \"bio1\",\n",
    "        \"type\": \"Exact technical terms (BM25 strength)\",\n",
    "        \"description\": \"Specific acronym and technical terms\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q5\",\n",
    "        \"query\": \"James Webb infrared telescope\",\n",
    "        \"expected\": \"space1\",\n",
    "        \"type\": \"Multi-keyword match (BM25 strength)\",\n",
    "        \"description\": \"All three keywords appear in target document\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q6\",\n",
    "        \"query\": \"AI that understands human language\",\n",
    "        \"expected\": \"ml3\",\n",
    "        \"type\": \"Paraphrasing (Embedding strength)\",\n",
    "        \"description\": \"Different words, same meaning as 'understand natural language'\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q7\",\n",
    "        \"query\": \"environmental impact of CO2\",\n",
    "        \"expected\": \"climate2\",\n",
    "        \"type\": \"Conceptual understanding (Embedding strength)\",\n",
    "        \"description\": \"CO2 ‚Üí greenhouse gas, conceptual link without exact words\"\n",
    "    },\n",
    "]\n",
    "\n",
    "df_queries = pd.DataFrame(queries)\n",
    "print(f\"üîç Queries: {len(queries)} test cases\\n\")\n",
    "df_queries[[\"id\", \"query\", \"type\", \"expected\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. BM25 Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Initializing BM25 retriever...\")\n",
    "bm25 = BM25()\n",
    "bm25.fit(corpus)\n",
    "print(\"‚úì BM25 ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Jina Embeddings Retriever\n",
    "\n",
    "**Note:** First run will download ~7.5GB model. Subsequent runs load from cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jina_retriever = None\n",
    "jina_embedder = None\n",
    "\n",
    "try:\n",
    "    print(\"üöÄ Loading Jina embeddings v4 model...\")\n",
    "    print(\"   (This may take a few minutes on first run)\\n\")\n",
    "    \n",
    "    jina_embedder = JinaEmbedder(model_name=\"jinaai/jina-embeddings-v4\", task=\"retrieval\")\n",
    "    print(\"‚úì Model loaded!\")\n",
    "    \n",
    "    # Test encoding\n",
    "    test_emb = jina_embedder.encode([\"test\"], prompt_name=\"passage\")\n",
    "    print(f\"‚úì Embedding dimension: {test_emb.shape[1]}\")\n",
    "    print(f\"‚úì Normalized: {np.allclose(np.linalg.norm(test_emb[0]), 1.0, rtol=1e-3)}\")\n",
    "    \n",
    "    jina_retriever = EmbeddingRetriever(embedder=jina_embedder)\n",
    "    jina_retriever.fit(corpus)\n",
    "    print(\"‚úì Jina retriever ready!\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading Jina model: {e}\")\n",
    "    print(\"   Install dependencies: pip install -r requirements.txt\")\n",
    "    print(\"   Continuing with BM25 only...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dummy Embeddings (Baseline)\n",
    "\n",
    "Random normalized embeddings for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üé≤ Initializing Dummy embedder (baseline)...\")\n",
    "dummy_embedder = DummyEmbedder()\n",
    "dummy_retriever = EmbeddingRetriever(embedder=dummy_embedder)\n",
    "dummy_retriever.fit(corpus)\n",
    "print(\"‚úì Dummy retriever ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Retrieval Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_retrieval(retriever, query, k=3, name=\"Retriever\"):\n",
    "    \"\"\"Run retrieval and return results.\"\"\"\n",
    "    if retriever is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        results = retriever.rank(query, k=k)\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {name}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def format_results(results, expected_doc=None):\n",
    "    \"\"\"Format results as dataframe with highlighting.\"\"\"\n",
    "    if results is None:\n",
    "        return None\n",
    "    \n",
    "    df = pd.DataFrame(results, columns=[\"doc_id\", \"score\"])\n",
    "    df[\"rank\"] = range(1, len(df) + 1)\n",
    "    df[\"correct\"] = df[\"doc_id\"] == expected_doc if expected_doc else False\n",
    "    df = df[[\"rank\", \"doc_id\", \"score\", \"correct\"]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Results\n",
    "\n",
    "For each query, we'll show:\n",
    "- Top 3 results from each retriever\n",
    "- Whether the expected document was retrieved\n",
    "- Analysis of why each method succeeded or failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results\n",
    "all_results = []\n",
    "\n",
    "for q in queries:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Query {q['id']}: {q['query']}\")\n",
    "    print(f\"Type: {q['type']}\")\n",
    "    print(f\"Expected: {q['expected']} - {q['description']}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # BM25\n",
    "    print(\"\\nüìä BM25 Results:\")\n",
    "    bm25_results = run_retrieval(bm25, q['query'], k=3, name=\"BM25\")\n",
    "    bm25_df = format_results(bm25_results, q['expected'])\n",
    "    if bm25_df is not None:\n",
    "        display(bm25_df)\n",
    "        bm25_correct = bm25_df[bm25_df['correct']].shape[0] > 0\n",
    "        bm25_rank = bm25_df[bm25_df['correct']]['rank'].values[0] if bm25_correct else None\n",
    "    else:\n",
    "        bm25_correct = False\n",
    "        bm25_rank = None\n",
    "    \n",
    "    # Jina Embeddings\n",
    "    print(\"\\nüöÄ Jina Embeddings Results:\")\n",
    "    jina_results = run_retrieval(jina_retriever, q['query'], k=3, name=\"Jina\")\n",
    "    jina_df = format_results(jina_results, q['expected'])\n",
    "    if jina_df is not None:\n",
    "        display(jina_df)\n",
    "        jina_correct = jina_df[jina_df['correct']].shape[0] > 0\n",
    "        jina_rank = jina_df[jina_df['correct']]['rank'].values[0] if jina_correct else None\n",
    "    else:\n",
    "        print(\"   (Not available)\")\n",
    "        jina_correct = False\n",
    "        jina_rank = None\n",
    "    \n",
    "    # Dummy (baseline)\n",
    "    print(\"\\nüé≤ Dummy Embeddings Results:\")\n",
    "    dummy_results = run_retrieval(dummy_retriever, q['query'], k=3, name=\"Dummy\")\n",
    "    dummy_df = format_results(dummy_results, q['expected'])\n",
    "    if dummy_df is not None:\n",
    "        display(dummy_df)\n",
    "        dummy_correct = dummy_df[dummy_df['correct']].shape[0] > 0\n",
    "        dummy_rank = dummy_df[dummy_df['correct']]['rank'].values[0] if dummy_correct else None\n",
    "    else:\n",
    "        dummy_correct = False\n",
    "        dummy_rank = None\n",
    "    \n",
    "    # Store results\n",
    "    all_results.append({\n",
    "        'query_id': q['id'],\n",
    "        'query': q['query'],\n",
    "        'type': q['type'],\n",
    "        'expected': q['expected'],\n",
    "        'bm25_correct': bm25_correct,\n",
    "        'bm25_rank': bm25_rank,\n",
    "        'jina_correct': jina_correct,\n",
    "        'jina_rank': jina_rank,\n",
    "        'dummy_correct': dummy_correct,\n",
    "        'dummy_rank': dummy_rank,\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SUMMARY: Retrieval Performance\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Overall accuracy\n",
    "print(\"\\nüìä Accuracy (found expected document in top 3):\")\n",
    "print(f\"  BM25:            {df_results['bm25_correct'].sum()}/{len(queries)} = {df_results['bm25_correct'].mean():.1%}\")\n",
    "if jina_retriever:\n",
    "    print(f\"  Jina Embeddings: {df_results['jina_correct'].sum()}/{len(queries)} = {df_results['jina_correct'].mean():.1%}\")\n",
    "print(f\"  Dummy (baseline): {df_results['dummy_correct'].sum()}/{len(queries)} = {df_results['dummy_correct'].mean():.1%}\")\n",
    "\n",
    "# By query type\n",
    "print(\"\\nüìà Performance by Query Type:\")\n",
    "display(df_results.groupby('type').agg({\n",
    "    'bm25_correct': 'mean',\n",
    "    'jina_correct': 'mean',\n",
    "    'dummy_correct': 'mean'\n",
    "}).round(2))\n",
    "\n",
    "print(\"\\nüí° Full Results Table:\")\n",
    "display(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "### BM25 Strengths:\n",
    "‚úÖ Excellent with **exact keyword matches**  \n",
    "‚úÖ Strong on **rare technical terms** (high IDF)  \n",
    "‚úÖ Fast and efficient  \n",
    "‚úÖ Interpretable (you can see which terms matched)  \n",
    "\n",
    "### BM25 Weaknesses:\n",
    "‚ùå No semantic understanding (\"global warming\" ‚â† \"greenhouse gas\")  \n",
    "‚ùå Struggles with **paraphrasing**  \n",
    "‚ùå Can't handle **conceptual queries**  \n",
    "‚ùå Vocabulary mismatch problems  \n",
    "\n",
    "### Jina Embeddings Strengths:\n",
    "‚úÖ Understands **semantic similarity**  \n",
    "‚úÖ Handles **paraphrasing** well  \n",
    "‚úÖ Works with **conceptual queries**  \n",
    "‚úÖ Robust to vocabulary mismatch  \n",
    "‚úÖ Multilingual and multimodal (text + images)  \n",
    "\n",
    "### Jina Embeddings Weaknesses:\n",
    "‚ùå Computationally expensive  \n",
    "‚ùå Requires GPU for fast inference at scale  \n",
    "‚ùå Less interpretable (black box)  \n",
    "‚ùå May miss exact matches if not in training data  \n",
    "\n",
    "### Best Practice:\n",
    "üéØ **Hybrid retrieval** - Combine both approaches!\n",
    "- Use BM25 for keyword matches\n",
    "- Use embeddings for semantic understanding\n",
    "- Merge and re-rank results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Inspection (Jina Embeddings)\n",
    "\n",
    "Let's visualize how Jina understands semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if jina_embedder:\n",
    "    print(\"üî¨ Pairwise Semantic Similarities\\n\")\n",
    "    \n",
    "    pairs = [\n",
    "        (\"climate change\", \"global warming\"),\n",
    "        (\"machine learning\", \"artificial intelligence\"),\n",
    "        (\"neural networks\", \"deep learning\"),\n",
    "        (\"quantum computing\", \"classical computing\"),\n",
    "        (\"climate change\", \"quantum computing\"),  # Unrelated\n",
    "    ]\n",
    "    \n",
    "    similarity_data = []\n",
    "    \n",
    "    for text1, text2 in pairs:\n",
    "        emb1 = jina_embedder.encode([text1], prompt_name=\"passage\")[0]\n",
    "        emb2 = jina_embedder.encode([text2], prompt_name=\"passage\")[0]\n",
    "        \n",
    "        # Cosine similarity (dot product for normalized vectors)\n",
    "        similarity = float(np.dot(emb1, emb2))\n",
    "        \n",
    "        similarity_data.append({\n",
    "            'text1': text1,\n",
    "            'text2': text2,\n",
    "            'similarity': similarity,\n",
    "            'interpretation': (\n",
    "                'Very similar' if similarity > 0.8 else\n",
    "                'Similar' if similarity > 0.6 else\n",
    "                'Somewhat similar' if similarity > 0.4 else\n",
    "                'Different'\n",
    "            )\n",
    "        })\n",
    "    \n",
    "    df_similarity = pd.DataFrame(similarity_data)\n",
    "    display(df_similarity)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Jina embeddings not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Both BM25 and neural embeddings have their place in modern IR systems:\n",
    "\n",
    "- **BM25**: Fast, interpretable, great for exact matches\n",
    "- **Embeddings**: Semantic understanding, handles paraphrasing\n",
    "- **Best approach**: Hybrid systems that combine both strengths\n",
    "\n",
    "For production systems, consider:\n",
    "1. First-stage retrieval with BM25 (fast, broad recall)\n",
    "2. Re-ranking with embeddings (precise, semantic)\n",
    "3. Evaluation metrics (Precision@k, MRR, NDCG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
